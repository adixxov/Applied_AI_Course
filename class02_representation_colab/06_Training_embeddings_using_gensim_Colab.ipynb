{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WGPwjhbbwPT"
      },
      "source": [
        "## Training Embeddings Using Gensim\n",
        "Word embeddings are an approach to representing text in NLP. In this notebook we will demonstrate how to train embeddings using Genism. [Gensim](https://radimrehurek.com/gensim/index.html) is an open source Python library for natural language processing, with a focus on topic modeling (explained in chapter 7)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install virtualenv\n",
        "!virtualenv ai_course_env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC2RxkewfS46",
        "outputId": "01d5927a-1bc9-4922-9e9f-e6ca2820c5fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: virtualenv in /usr/local/lib/python3.10/dist-packages (20.26.4)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (0.3.8)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (3.16.0)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv) (4.3.2)\n",
            "created virtual environment CPython3.10.12.final.0-64 in 740ms\n",
            "  creator CPython3Posix(dest=/content/ai_course_env, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: certifi==2024.8.30, chardet==3.0.4, gensim==3.6.0, idna==2.10, numpy==2.1.1, pip==24.2, requests==2.23.0, scipy==1.14.1, setuptools==74.1.2, six==1.16.0, smart_open==7.0.4, urllib3==1.25.11, wheel==0.44.0, wrapt==1.16.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyXXFeFZ750T",
        "outputId": "290d4f3b-8330-47f0-b09b-3db4baaab014"
      },
      "source": [
        "!source /content/ai_course_env/bin/activate; pip install gensim==3.6.0\n",
        "!source /content/ai_course_env/bin/activate; pip install requests==2.23.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==3.6.0 in ./ai_course_env/lib/python3.10/site-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in ./ai_course_env/lib/python3.10/site-packages (from gensim==3.6.0) (2.1.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in ./ai_course_env/lib/python3.10/site-packages (from gensim==3.6.0) (1.14.1)\n",
            "Requirement already satisfied: six>=1.5.0 in ./ai_course_env/lib/python3.10/site-packages (from gensim==3.6.0) (1.16.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in ./ai_course_env/lib/python3.10/site-packages (from gensim==3.6.0) (7.0.4)\n",
            "Requirement already satisfied: wrapt in ./ai_course_env/lib/python3.10/site-packages (from smart-open>=1.2.1->gensim==3.6.0) (1.16.0)\n",
            "Requirement already satisfied: requests==2.23.0 in ./ai_course_env/lib/python3.10/site-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in ./ai_course_env/lib/python3.10/site-packages (from requests==2.23.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in ./ai_course_env/lib/python3.10/site-packages (from requests==2.23.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./ai_course_env/lib/python3.10/site-packages (from requests==2.23.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./ai_course_env/lib/python3.10/site-packages (from requests==2.23.0) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBw9OCYcYQ_n",
        "ExecuteTime": {
          "end_time": "2024-09-07T17:11:37.229390Z",
          "start_time": "2024-09-07T17:11:35.481768Z"
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qWptd54ZcfV",
        "ExecuteTime": {
          "end_time": "2024-09-07T17:11:39.711414Z",
          "start_time": "2024-09-07T17:11:39.668664Z"
        }
      },
      "source": [
        "# define training data\n",
        "#Genism word2vec requires that a format of ‘list of lists’ be provided for training where every document contained in a list.\n",
        "#Every list contains lists of tokens of that document.\n",
        "corpus = [['dog','bites','man'], [\"man\", \"bites\" ,\"dog\"],[\"dog\",\"eats\",\"meat\"],[\"man\", \"eats\",\"food\"]]\n",
        "\n",
        "#Training the model\n",
        "model_cbow = Word2Vec(corpus, min_count=1,sg=0) #using CBOW Architecture for trainnig\n",
        "model_skipgram = Word2Vec(corpus, min_count=1,sg=1)#using skipGram Architecture for training"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "dir(model_cbow.wv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQMPR3vUg6Q7",
        "outputId": "b3fc2059-301b-493a-896a-058d6e4b0115"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__contains__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getitem__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__setitem__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_adapt_by_suffix',\n",
              " '_load_specials',\n",
              " '_log_evaluate_word_analogies',\n",
              " '_save_specials',\n",
              " '_smart_save',\n",
              " '_upconvert_old_d2vkv',\n",
              " '_upconvert_old_vocab',\n",
              " 'add_lifecycle_event',\n",
              " 'add_vector',\n",
              " 'add_vectors',\n",
              " 'allocate_vecattrs',\n",
              " 'closer_than',\n",
              " 'cosine_similarities',\n",
              " 'distance',\n",
              " 'distances',\n",
              " 'doesnt_match',\n",
              " 'evaluate_word_analogies',\n",
              " 'evaluate_word_pairs',\n",
              " 'expandos',\n",
              " 'fill_norms',\n",
              " 'get_index',\n",
              " 'get_mean_vector',\n",
              " 'get_normed_vectors',\n",
              " 'get_vecattr',\n",
              " 'get_vector',\n",
              " 'has_index_for',\n",
              " 'index2entity',\n",
              " 'index2word',\n",
              " 'index_to_key',\n",
              " 'init_sims',\n",
              " 'intersect_word2vec_format',\n",
              " 'key_to_index',\n",
              " 'load',\n",
              " 'load_word2vec_format',\n",
              " 'log_accuracy',\n",
              " 'log_evaluate_word_pairs',\n",
              " 'mapfile_path',\n",
              " 'most_similar',\n",
              " 'most_similar_cosmul',\n",
              " 'most_similar_to_given',\n",
              " 'n_similarity',\n",
              " 'next_index',\n",
              " 'norms',\n",
              " 'rank',\n",
              " 'rank_by_centrality',\n",
              " 'relative_cosine_similarity',\n",
              " 'resize_vectors',\n",
              " 'save',\n",
              " 'save_word2vec_format',\n",
              " 'set_vecattr',\n",
              " 'similar_by_key',\n",
              " 'similar_by_vector',\n",
              " 'similar_by_word',\n",
              " 'similarity',\n",
              " 'similarity_unseen_docs',\n",
              " 'sort_by_descending_frequency',\n",
              " 'unit_normalize_all',\n",
              " 'vector_size',\n",
              " 'vectors',\n",
              " 'vectors_for_all',\n",
              " 'vectors_lockf',\n",
              " 'vectors_norm',\n",
              " 'vocab',\n",
              " 'wmdistance',\n",
              " 'word_vec',\n",
              " 'words_closer_than']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model_cbow.wv.get_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9obBDlTwiJg2",
        "outputId": "317f8983-e2de-4ed0-c3af-2e6aa801316b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "method"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyZY8ME4lUjd",
        "outputId": "50713f7a-fac0-4b23-bbfe-9f93d35d1804",
        "ExecuteTime": {
          "end_time": "2024-09-07T17:12:16.287637Z",
          "start_time": "2024-09-07T17:12:16.277073Z"
        }
      },
      "source": [
        "#Summarize the loaded model\n",
        "print(model_cbow)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(model_cbow.wv.key_to_index)\n",
        "print(words)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(model_cbow.wv.get_vector('dog'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n",
            "['man', 'dog', 'eats', 'bites', 'food', 'meat']\n",
            "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMuHv52GeuoR",
        "outputId": "eab492cc-e0ed-408e-d79e-9d0e994ea04e",
        "ExecuteTime": {
          "end_time": "2024-09-07T17:13:18.872194Z",
          "start_time": "2024-09-07T17:13:18.861141Z"
        }
      },
      "source": [
        "#Compute similarity\n",
        "print(\"Similarity between eats and bites:\",model_cbow.wv.similarity('eats', 'bites'))\n",
        "print(\"Similarity between eats and man:\",model_cbow.wv.similarity('eats', 'man'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between eats and bites: -0.0134970825\n",
            "Similarity between eats and man: -0.05235437\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twhTZfPOezTU"
      },
      "source": [
        "From the above similarity scores we can conclude that eats is more similar to bites than man."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lv0V7WofmsB",
        "outputId": "ffab776e-e862-4058-e38b-7f4f06926f0a",
        "ExecuteTime": {
          "end_time": "2024-09-07T17:13:32.073760Z",
          "start_time": "2024-09-07T17:13:32.065310Z"
        }
      },
      "source": [
        "#Most similarity\n",
        "model_cbow.wv.most_similar('meat')"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('food', 0.13887985050678253),\n",
              " ('bites', 0.13149003684520721),\n",
              " ('eats', 0.06422408670186996),\n",
              " ('dog', 0.009391166269779205),\n",
              " ('man', -0.05987630784511566)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-05T21:26:59.855822Z",
          "start_time": "2021-04-05T21:26:59.841810Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA783nrSalgs",
        "outputId": "db092412-8237-473d-b745-c8e9f7187472"
      },
      "source": [
        "# save model\n",
        "model_cbow.save('model_cbow.bin')\n",
        "\n",
        "# load model\n",
        "new_model_cbow = Word2Vec.load('model_cbow.bin')\n",
        "print(new_model_cbow)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deReLSI7mQyr"
      },
      "source": [
        "## SkipGram\n",
        "In skipgram, the task is to predict the context words from the center word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-05T21:27:00.517046Z",
          "start_time": "2021-04-05T21:27:00.508038Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QtUtsLglvY0",
        "outputId": "e5a379fe-f584-4b85-f114-7dbe218cd29e"
      },
      "source": [
        "#Summarize the loaded model\n",
        "print(model_skipgram)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(model_skipgram.wv.index_to_key)\n",
        "print(words)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(model_skipgram.wv['dog'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n",
            "['man', 'dog', 'eats', 'bites', 'food', 'meat']\n",
            "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-05T21:27:02.660747Z",
          "start_time": "2021-04-05T21:27:02.642866Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YUsblEOfFWf",
        "outputId": "81bebc98-5857-445e-820e-ef5d10ecbd11"
      },
      "source": [
        "#Compute similarity\n",
        "print(\"Similarity between eats and bites:\",model_skipgram.wv.similarity('eats', 'bites'))\n",
        "print(\"Similarity between eats and man:\",model_skipgram.wv.similarity('eats', 'man'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between eats and bites: -0.013518792\n",
            "Similarity between eats and man: -0.05234511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdXVDePKnBpv"
      },
      "source": [
        "From the above similarity scores we can conclude that eats is more similar to bites than man."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-05T21:27:03.419546Z",
          "start_time": "2021-04-05T21:27:03.414541Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpF4qtwpmuM3",
        "outputId": "2f3be96e-b0d2-4312-a818-00c5cc16518b"
      },
      "source": [
        "#Most similarity\n",
        "model_skipgram.wv.most_similar('meat')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('food', 0.13887983560562134),\n",
              " ('bites', 0.13149002194404602),\n",
              " ('eats', 0.06406080722808838),\n",
              " ('dog', 0.009391166269779205),\n",
              " ('man', -0.059876300394535065)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-05T21:27:03.973454Z",
          "start_time": "2021-04-05T21:27:03.950433Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNDCEXRTnAnj",
        "outputId": "d621aeee-447b-4d0b-b4da-7a4ba86c23e3"
      },
      "source": [
        "# save model\n",
        "model_skipgram.save('model_skipgram.bin')\n",
        "\n",
        "# load model\n",
        "new_model_skipgram = Word2Vec.load('model_skipgram.bin')\n",
        "print(new_model_skipgram)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=6, vector_size=100, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0MiqJ_1M0mX"
      },
      "source": [
        "## Training Your Embedding on Wiki Corpus\n",
        "\n",
        "##### The corpus download page : https://dumps.wikimedia.org/enwiki/20200120/\n",
        "The entire wiki corpus as of 28/04/2020 is just over 16GB in size.\n",
        "We will take a part of this corpus due to computation constraints and train our word2vec and fasttext embeddings.\n",
        "\n",
        "The file size is 294MB so it can take a while to download.\n",
        "\n",
        "Source for code which downloads files from Google Drive: https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-05T21:27:58.596845Z",
          "start_time": "2021-04-05T21:27:58.585833Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLq8kxmF750d",
        "outputId": "f9307df3-494c-4426-d681-fce7f6908651"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "os.makedirs('data/en', exist_ok= True)\n",
        "file_name = \"data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\"\n",
        "file_id = \"11804g0GcWnBIVDahjo5fQyc05nQLXGwF\"\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "if not os.path.exists(file_name):\n",
        "    download_file_from_google_drive(file_id, file_name)\n",
        "else:\n",
        "    print(\"file already exists, skipping download\")\n",
        "\n",
        "print(f\"File at: {file_name}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file already exists, skipping download\n",
            "File at: data/en/enwiki-latest-pages-articles-multistream14.xml-p13159683p14324602.bz2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T08:59:17.024306Z",
          "start_time": "2021-04-03T08:59:17.022304Z"
        },
        "id": "wX1kx96JLYvt"
      },
      "source": [
        "from gensim.corpora.wikicorpus import WikiCorpus\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.fasttext import FastText\n",
        "import time"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "ipcvvk6ybAU1",
        "outputId": "97a00615-284e-46c7-b47a-4db3b63141ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T09:56:14.722195Z",
          "start_time": "2021-04-03T09:56:14.705177Z"
        },
        "id": "rJgsEUmRPppc",
        "outputId": "7564eb9a-5375-49c1-8ed2-56e7ff092dfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Preparing the Training data\n",
        "import spacy\n",
        "# Load spaCy's English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "wiki = WikiCorpus(file_name, dictionary={})\n",
        "sentences = list(wiki.get_texts())\n",
        "\n",
        "# Apply lemmatization on each sentence\n",
        "lemmatized_sentences = []\n",
        "for sentence in sentences:\n",
        "    doc = nlp(\" \".join(sentence))  # Convert sentence list to string\n",
        "    lemmatized_sentence = [token.lemma_ for token in doc]\n",
        "    lemmatized_sentences.append(lemmatized_sentence)\n",
        "\n",
        "#if you get a memory error executing the lines above\n",
        "#comment the lines out and uncomment the lines below.\n",
        "#loading will be slower, but stable.\n",
        "# wiki = WikiCorpus(file_name, processes=4, lemmatize=False, dictionary={})\n",
        "# sentences = list(wiki.get_texts())\n",
        "\n",
        "#if you still get a memory error, try settings processes to 1 or 2 and then run it again."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process InputQueue-4:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gensim/utils.py\", line 1291, in run\n",
            "    wrapped_chunk = [list(chunk)]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gensim/corpora/wikicorpus.py\", line 682, in <genexpr>\n",
            "    texts = (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gensim/corpora/wikicorpus.py\", line 412, in extract_pages\n",
            "    elem = next(elems)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gensim/corpora/wikicorpus.py\", line 406, in <genexpr>\n",
            "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
            "  File \"/usr/lib/python3.10/xml/etree/ElementTree.py\", line 1255, in iterator\n",
            "    data = source.read(16 * 1024)\n",
            "  File \"/usr/lib/python3.10/bz2.py\", line 164, in read\n",
            "    return self._buffer.read(size)\n",
            "  File \"/usr/lib/python3.10/_compression.py\", line 68, in readinto\n",
            "    data = self.read(len(byte_view))\n",
            "  File \"/usr/lib/python3.10/_compression.py\", line 103, in read\n",
            "    data = self._decompressor.decompress(rawblock, size)\n",
            "OSError: Invalid data stream\n",
            "WARNING:gensim.corpora.wikicorpus:user terminated iteration over Wikipedia corpus after 0 documents with 0 positions (total 0 articles, 0 positions before pruning articles shorter than 50 words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsIrgt_gPQda"
      },
      "source": [
        "### Hyperparameters\n",
        "\n",
        "\n",
        "1.   sg - Selecting the training algorithm: 1 for skip-gram else its 0 for CBOW. Default is CBOW.\n",
        "2.   min_count-  Ignores all words with total frequency lower than this.<br>\n",
        "There are many more hyperparamaeters whose list can be found in the official documentation [here.](https://radimrehurek.com/gensim/models/word2vec.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:01:20.065332Z",
          "start_time": "2021-04-03T09:59:12.350872Z"
        },
        "id": "idmfbr_8LvoN"
      },
      "source": [
        "#CBOW\n",
        "start = time.time()\n",
        "word2vec_cbow = Word2Vec(sentences,min_count=10, sg=0)\n",
        "end = time.time()\n",
        "\n",
        "print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:02:10.613551Z",
          "start_time": "2021-04-03T10:02:10.585535Z"
        },
        "id": "mMdGn08-RkhM"
      },
      "source": [
        "#Summarize the loaded model\n",
        "print(word2vec_cbow)\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(word2vec_cbow.wv.vocab)\n",
        "print(f\"Length of vocabulary: {len(words)}\")\n",
        "print(\"Printing the first 30 words.\")\n",
        "print(words[:30])\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(f\"Length of vector: {len(word2vec_cbow['film'])}\")\n",
        "print(word2vec_cbow['film'])\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Compute similarity\n",
        "print(\"Similarity between film and drama:\",word2vec_cbow.similarity('film', 'drama'))\n",
        "print(\"Similarity between film and tiger:\",word2vec_cbow.similarity('film', 'tiger'))\n",
        "print(\"-\"*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:02:16.109851Z",
          "start_time": "2021-04-03T10:02:15.257052Z"
        },
        "id": "rXrDOrKskcHX"
      },
      "source": [
        "# save model\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "word2vec_cbow.wv.save_word2vec_format('word2vec_cbow.bin', binary=True)\n",
        "\n",
        "# load model\n",
        "# new_modelword2vec_cbow = Word2Vec.load('word2vec_cbow.bin')\n",
        "# print(word2vec_cbow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:08:27.736688Z",
          "start_time": "2021-04-03T10:02:19.197708Z"
        },
        "id": "dX0U0CbQOK30"
      },
      "source": [
        "#SkipGram\n",
        "start = time.time()\n",
        "word2vec_skipgram = Word2Vec(sentences,min_count=10, sg=1)\n",
        "end = time.time()\n",
        "\n",
        "print(\"SkipGram Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:09:06.406929Z",
          "start_time": "2021-04-03T10:09:06.383908Z"
        },
        "id": "LXnY9YInSvnI"
      },
      "source": [
        "#Summarize the loaded model\n",
        "print(word2vec_skipgram)\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(word2vec_skipgram.wv.vocab)\n",
        "print(f\"Length of vocabulary: {len(words)}\")\n",
        "print(\"Printing the first 30 words.\")\n",
        "print(words[:30])\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(f\"Length of vector: {len(word2vec_skipgram['film'])}\")\n",
        "print(word2vec_skipgram['film'])\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Compute similarity\n",
        "print(\"Similarity between film and drama:\",word2vec_skipgram.similarity('film', 'drama'))\n",
        "print(\"Similarity between film and tiger:\",word2vec_skipgram.similarity('film', 'tiger'))\n",
        "print(\"-\"*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:09:09.947695Z",
          "start_time": "2021-04-03T10:09:09.076901Z"
        },
        "id": "o8U7bfPSVB04"
      },
      "source": [
        "# save model\n",
        "word2vec_skipgram.wv.save_word2vec_format('word2vec_sg.bin', binary=True)\n",
        "\n",
        "# load model\n",
        "# new_model_skipgram = Word2Vec.load('model_skipgram.bin')\n",
        "# print(model_skipgram)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kExlA8kfrKml"
      },
      "source": [
        "## FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:16:31.271764Z",
          "start_time": "2021-04-03T10:09:16.592670Z"
        },
        "id": "JPd2VhMEk8gL"
      },
      "source": [
        "#CBOW\n",
        "start = time.time()\n",
        "fasttext_cbow = FastText(sentences, sg=0, min_count=10)\n",
        "end = time.time()\n",
        "\n",
        "print(\"FastText CBOW Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:16:31.287283Z",
          "start_time": "2021-04-03T10:16:31.273765Z"
        },
        "id": "FlQFl8-Zsost"
      },
      "source": [
        "#Summarize the loaded model\n",
        "print(fasttext_cbow)\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(fasttext_cbow.wv.vocab)\n",
        "print(f\"Length of vocabulary: {len(words)}\")\n",
        "print(\"Printing the first 30 words.\")\n",
        "print(words[:30])\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(f\"Length of vector: {len(fasttext_cbow['film'])}\")\n",
        "print(fasttext_cbow['film'])\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Compute similarity\n",
        "print(\"Similarity between film and drama:\",fasttext_cbow.similarity('film', 'drama'))\n",
        "print(\"Similarity between film and tiger:\",fasttext_cbow.similarity('film', 'tiger'))\n",
        "print(\"-\"*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:28:28.771383Z",
          "start_time": "2021-04-03T10:16:31.289284Z"
        },
        "id": "UgSOxsNklAvh"
      },
      "source": [
        "#SkipGram\n",
        "start = time.time()\n",
        "fasttext_skipgram = FastText(sentences, sg=1, min_count=10)\n",
        "end = time.time()\n",
        "\n",
        "print(\"FastText SkipGram Model Training Complete\\nTime taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-04-03T10:28:28.803412Z",
          "start_time": "2021-04-03T10:28:28.773386Z"
        },
        "id": "vFiTAP0PsQwi"
      },
      "source": [
        "#Summarize the loaded model\n",
        "print(fasttext_skipgram)\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Summarize vocabulary\n",
        "words = list(fasttext_skipgram.wv.vocab)\n",
        "print(f\"Length of vocabulary: {len(words)}\")\n",
        "print(\"Printing the first 30 words.\")\n",
        "print(words[:30])\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Acess vector for one word\n",
        "print(f\"Length of vector: {len(fasttext_skipgram['film'])}\")\n",
        "print(fasttext_skipgram['film'])\n",
        "print(\"-\"*30)\n",
        "\n",
        "#Compute similarity\n",
        "print(\"Similarity between film and drama:\",fasttext_skipgram.similarity('film', 'drama'))\n",
        "print(\"Similarity between film and tiger:\",fasttext_skipgram.similarity('film', 'tiger'))\n",
        "print(\"-\"*30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oArMIJzYOmUR"
      },
      "source": [
        "#### An interesting obeseravtion if you noticed is that CBOW trains faster than SkipGram in both cases.\n",
        "We will leave it to the user to figure out why. A hint would be to refer the working of CBOW and skipgram."
      ]
    }
  ]
}