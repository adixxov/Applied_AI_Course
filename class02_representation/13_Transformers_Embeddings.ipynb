{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007274150848388672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea621c367e404e39a3377a983c0f9a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# laod model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|> Subject: Sincere Apologies for the Gardening Mishap\n",
      "\n",
      "\n",
      "Dear\n"
     ]
    }
   ],
   "source": [
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=20\n",
    ")\n",
    "\n",
    "# Print the output\n",
    "print(tokenizer.decode(generation_output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14350,   385,  4876, 27746,  5281,   304, 19235,   363,   278, 25305,\n",
      "          293, 16423,   292,   286,   728,   481, 29889, 12027,  7420,   920,\n",
      "          372,  9559, 29889, 32001,  3323,   622, 29901,   317,  3742,   406,\n",
      "         6225, 11763,   363,   278, 19906,   292,   341,   728,   481,    13,\n",
      "           13,    13, 29928,   799], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(generation_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "ߎ堩蟼\n",
    "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
    "12.0*50=600\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_list = [\n",
    "    (102, 194, 165), (252, 141, 98), (141, 160, 203),\n",
    "    (231, 138, 195), (166, 216, 84), (255, 217, 47)\n",
    "]\n",
    "\n",
    "def show_tokens(sentence, tokenizer_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        r, g, b = colors_list[idx % len(colors_list)]\n",
    "        color_code = f\"\\033[48;2;{r};{g};{b}m\"\n",
    "        reset_code = \"\\033[0m\"\n",
    "        print(f\"{color_code}{tokenizer.decode(t)}{reset_code}\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;102;194;165m[CLS]\u001b[0m \u001b[48;2;252;141;98menglish\u001b[0m \u001b[48;2;141;160;203mand\u001b[0m \u001b[48;2;231;138;195mcapital\u001b[0m \u001b[48;2;166;216;84m##ization\u001b[0m \u001b[48;2;255;217;47m[UNK]\u001b[0m \u001b[48;2;102;194;165m[UNK]\u001b[0m \u001b[48;2;252;141;98m[UNK]\u001b[0m \u001b[48;2;141;160;203mshow\u001b[0m \u001b[48;2;231;138;195m_\u001b[0m \u001b[48;2;166;216;84mtoken\u001b[0m \u001b[48;2;255;217;47m##s\u001b[0m \u001b[48;2;102;194;165mfalse\u001b[0m \u001b[48;2;252;141;98mnone\u001b[0m \u001b[48;2;141;160;203meli\u001b[0m \u001b[48;2;231;138;195m##f\u001b[0m \u001b[48;2;166;216;84m=\u001b[0m \u001b[48;2;255;217;47m=\u001b[0m \u001b[48;2;102;194;165m>\u001b[0m \u001b[48;2;252;141;98m=\u001b[0m \u001b[48;2;141;160;203melse\u001b[0m \u001b[48;2;231;138;195m:\u001b[0m \u001b[48;2;166;216;84mtwo\u001b[0m \u001b[48;2;255;217;47mtab\u001b[0m \u001b[48;2;102;194;165m##s\u001b[0m \u001b[48;2;252;141;98m:\u001b[0m \u001b[48;2;141;160;203m\"\u001b[0m \u001b[48;2;231;138;195m\"\u001b[0m \u001b[48;2;166;216;84mthree\u001b[0m \u001b[48;2;255;217;47mtab\u001b[0m \u001b[48;2;102;194;165m##s\u001b[0m \u001b[48;2;252;141;98m:\u001b[0m \u001b[48;2;141;160;203m\"\u001b[0m \u001b[48;2;231;138;195m\"\u001b[0m \u001b[48;2;166;216;84m12\u001b[0m \u001b[48;2;255;217;47m.\u001b[0m \u001b[48;2;102;194;165m0\u001b[0m \u001b[48;2;252;141;98m*\u001b[0m \u001b[48;2;141;160;203m50\u001b[0m \u001b[48;2;231;138;195m=\u001b[0m \u001b[48;2;166;216;84m600\u001b[0m \u001b[48;2;255;217;47m[SEP]\u001b[0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;102;194;165m\n",
      "\u001b[0m \u001b[48;2;252;141;98mEnglish\u001b[0m \u001b[48;2;141;160;203m and\u001b[0m \u001b[48;2;231;138;195m CAP\u001b[0m \u001b[48;2;166;216;84mITAL\u001b[0m \u001b[48;2;255;217;47mIZ\u001b[0m \u001b[48;2;102;194;165mATION\u001b[0m \u001b[48;2;252;141;98m\n",
      "\u001b[0m \u001b[48;2;141;160;203m�\u001b[0m \u001b[48;2;231;138;195m�\u001b[0m \u001b[48;2;166;216;84m�\u001b[0m \u001b[48;2;255;217;47m�\u001b[0m \u001b[48;2;102;194;165m�\u001b[0m \u001b[48;2;252;141;98m�\u001b[0m \u001b[48;2;141;160;203m�\u001b[0m \u001b[48;2;231;138;195m�\u001b[0m \u001b[48;2;166;216;84m\n",
      "\u001b[0m \u001b[48;2;255;217;47mshow\u001b[0m \u001b[48;2;102;194;165m_\u001b[0m \u001b[48;2;252;141;98mt\u001b[0m \u001b[48;2;141;160;203mok\u001b[0m \u001b[48;2;231;138;195mens\u001b[0m \u001b[48;2;166;216;84m False\u001b[0m \u001b[48;2;255;217;47m None\u001b[0m \u001b[48;2;102;194;165m el\u001b[0m \u001b[48;2;252;141;98mif\u001b[0m \u001b[48;2;141;160;203m ==\u001b[0m \u001b[48;2;231;138;195m >=\u001b[0m \u001b[48;2;166;216;84m else\u001b[0m \u001b[48;2;255;217;47m:\u001b[0m \u001b[48;2;102;194;165m two\u001b[0m \u001b[48;2;252;141;98m tabs\u001b[0m \u001b[48;2;141;160;203m:\"\u001b[0m \u001b[48;2;231;138;195m \u001b[0m \u001b[48;2;166;216;84m \u001b[0m \u001b[48;2;255;217;47m \u001b[0m \u001b[48;2;102;194;165m \"\u001b[0m \u001b[48;2;252;141;98m Three\u001b[0m \u001b[48;2;141;160;203m tabs\u001b[0m \u001b[48;2;231;138;195m:\u001b[0m \u001b[48;2;166;216;84m \"\u001b[0m \u001b[48;2;255;217;47m \u001b[0m \u001b[48;2;102;194;165m \u001b[0m \u001b[48;2;252;141;98m \u001b[0m \u001b[48;2;141;160;203m \u001b[0m \u001b[48;2;231;138;195m \u001b[0m \u001b[48;2;166;216;84m \u001b[0m \u001b[48;2;255;217;47m \"\u001b[0m \u001b[48;2;102;194;165m\n",
      "\u001b[0m \u001b[48;2;252;141;98m12\u001b[0m \u001b[48;2;141;160;203m.\u001b[0m \u001b[48;2;231;138;195m0\u001b[0m \u001b[48;2;166;216;84m*\u001b[0m \u001b[48;2;255;217;47m50\u001b[0m \u001b[48;2;102;194;165m=\u001b[0m \u001b[48;2;252;141;98m600\u001b[0m \u001b[48;2;141;160;203m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;102;194;165mEnglish\u001b[0m \u001b[48;2;252;141;98mand\u001b[0m \u001b[48;2;141;160;203mCA\u001b[0m \u001b[48;2;231;138;195mPI\u001b[0m \u001b[48;2;166;216;84mTAL\u001b[0m \u001b[48;2;255;217;47mIZ\u001b[0m \u001b[48;2;102;194;165mATION\u001b[0m \u001b[48;2;252;141;98m\u001b[0m \u001b[48;2;141;160;203m<unk>\u001b[0m \u001b[48;2;231;138;195mshow\u001b[0m \u001b[48;2;166;216;84m_\u001b[0m \u001b[48;2;255;217;47mto\u001b[0m \u001b[48;2;102;194;165mken\u001b[0m \u001b[48;2;252;141;98ms\u001b[0m \u001b[48;2;141;160;203mFal\u001b[0m \u001b[48;2;231;138;195ms\u001b[0m \u001b[48;2;166;216;84me\u001b[0m \u001b[48;2;255;217;47mNone\u001b[0m \u001b[48;2;102;194;165m\u001b[0m \u001b[48;2;252;141;98me\u001b[0m \u001b[48;2;141;160;203ml\u001b[0m \u001b[48;2;231;138;195mif\u001b[0m \u001b[48;2;166;216;84m=\u001b[0m \u001b[48;2;255;217;47m=\u001b[0m \u001b[48;2;102;194;165m>\u001b[0m \u001b[48;2;252;141;98m=\u001b[0m \u001b[48;2;141;160;203melse\u001b[0m \u001b[48;2;231;138;195m:\u001b[0m \u001b[48;2;166;216;84mtwo\u001b[0m \u001b[48;2;255;217;47mtab\u001b[0m \u001b[48;2;102;194;165ms\u001b[0m \u001b[48;2;252;141;98m:\u001b[0m \u001b[48;2;141;160;203m\"\u001b[0m \u001b[48;2;231;138;195m\"\u001b[0m \u001b[48;2;166;216;84mThree\u001b[0m \u001b[48;2;255;217;47mtab\u001b[0m \u001b[48;2;102;194;165ms\u001b[0m \u001b[48;2;252;141;98m:\u001b[0m \u001b[48;2;141;160;203m\"\u001b[0m \u001b[48;2;231;138;195m\"\u001b[0m \u001b[48;2;166;216;84m12.\u001b[0m \u001b[48;2;255;217;47m0\u001b[0m \u001b[48;2;102;194;165m*\u001b[0m \u001b[48;2;252;141;98m50\u001b[0m \u001b[48;2;141;160;203m=\u001b[0m \u001b[48;2;231;138;195m600\u001b[0m \u001b[48;2;166;216;84m\u001b[0m \u001b[48;2;255;217;47m</s>\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"google/flan-t5-xxl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;102;194;165m\n",
      "\u001b[0m \u001b[48;2;252;141;98mEnglish\u001b[0m \u001b[48;2;141;160;203m and\u001b[0m \u001b[48;2;231;138;195m CAPITAL\u001b[0m \u001b[48;2;166;216;84mIZATION\u001b[0m \u001b[48;2;255;217;47m\n",
      "\u001b[0m \u001b[48;2;102;194;165m�\u001b[0m \u001b[48;2;252;141;98m�\u001b[0m \u001b[48;2;141;160;203m�\u001b[0m \u001b[48;2;231;138;195m�\u001b[0m \u001b[48;2;166;216;84m�\u001b[0m \u001b[48;2;255;217;47m�\u001b[0m \u001b[48;2;102;194;165m�\u001b[0m \u001b[48;2;252;141;98m\n",
      "\u001b[0m \u001b[48;2;141;160;203mshow\u001b[0m \u001b[48;2;231;138;195m_tokens\u001b[0m \u001b[48;2;166;216;84m False\u001b[0m \u001b[48;2;255;217;47m None\u001b[0m \u001b[48;2;102;194;165m elif\u001b[0m \u001b[48;2;252;141;98m ==\u001b[0m \u001b[48;2;141;160;203m >=\u001b[0m \u001b[48;2;231;138;195m else\u001b[0m \u001b[48;2;166;216;84m:\u001b[0m \u001b[48;2;255;217;47m two\u001b[0m \u001b[48;2;102;194;165m tabs\u001b[0m \u001b[48;2;252;141;98m:\"\u001b[0m \u001b[48;2;141;160;203m   \u001b[0m \u001b[48;2;231;138;195m \"\u001b[0m \u001b[48;2;166;216;84m Three\u001b[0m \u001b[48;2;255;217;47m tabs\u001b[0m \u001b[48;2;102;194;165m:\u001b[0m \u001b[48;2;252;141;98m \"\u001b[0m \u001b[48;2;141;160;203m      \u001b[0m \u001b[48;2;231;138;195m \"\n",
      "\u001b[0m \u001b[48;2;166;216;84m12\u001b[0m \u001b[48;2;255;217;47m.\u001b[0m \u001b[48;2;102;194;165m0\u001b[0m \u001b[48;2;252;141;98m*\u001b[0m \u001b[48;2;141;160;203m50\u001b[0m \u001b[48;2;231;138;195m=\u001b[0m \u001b[48;2;166;216;84m600\u001b[0m \u001b[48;2;255;217;47m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"Xenova/gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;102;194;165m\n",
      "\u001b[0m \u001b[48;2;252;141;98mEnglish\u001b[0m \u001b[48;2;141;160;203m and\u001b[0m \u001b[48;2;231;138;195m CAPITAL\u001b[0m \u001b[48;2;166;216;84mIZATION\u001b[0m \u001b[48;2;255;217;47m\n",
      "\u001b[0m \u001b[48;2;102;194;165m�\u001b[0m \u001b[48;2;252;141;98m�\u001b[0m \u001b[48;2;141;160;203m�\u001b[0m \u001b[48;2;231;138;195m�\u001b[0m \u001b[48;2;166;216;84m�\u001b[0m \u001b[48;2;255;217;47m�\u001b[0m \u001b[48;2;102;194;165m�\u001b[0m \u001b[48;2;252;141;98m\n",
      "\u001b[0m \u001b[48;2;141;160;203mshow\u001b[0m \u001b[48;2;231;138;195m_\u001b[0m \u001b[48;2;166;216;84mtokens\u001b[0m \u001b[48;2;255;217;47m False\u001b[0m \u001b[48;2;102;194;165m None\u001b[0m \u001b[48;2;252;141;98m elif\u001b[0m \u001b[48;2;141;160;203m ==\u001b[0m \u001b[48;2;231;138;195m >=\u001b[0m \u001b[48;2;166;216;84m else\u001b[0m \u001b[48;2;255;217;47m:\u001b[0m \u001b[48;2;102;194;165m two\u001b[0m \u001b[48;2;252;141;98m tabs\u001b[0m \u001b[48;2;141;160;203m:\"\u001b[0m \u001b[48;2;231;138;195m   \u001b[0m \u001b[48;2;166;216;84m \"\u001b[0m \u001b[48;2;255;217;47m Three\u001b[0m \u001b[48;2;102;194;165m tabs\u001b[0m \u001b[48;2;252;141;98m:\u001b[0m \u001b[48;2;141;160;203m \"\u001b[0m \u001b[48;2;231;138;195m      \u001b[0m \u001b[48;2;166;216;84m \"\u001b[0m \u001b[48;2;255;217;47m\n",
      "\u001b[0m \u001b[48;2;102;194;165m1\u001b[0m \u001b[48;2;252;141;98m2\u001b[0m \u001b[48;2;141;160;203m.\u001b[0m \u001b[48;2;231;138;195m0\u001b[0m \u001b[48;2;166;216;84m*\u001b[0m \u001b[48;2;255;217;47m5\u001b[0m \u001b[48;2;102;194;165m0\u001b[0m \u001b[48;2;252;141;98m=\u001b[0m \u001b[48;2;141;160;203m6\u001b[0m \u001b[48;2;231;138;195m0\u001b[0m \u001b[48;2;166;216;84m0\u001b[0m \u001b[48;2;255;217;47m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bigcode/starcoder2-15b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;102;194;165m\n",
      "\u001b[0m \u001b[48;2;252;141;98mEnglish\u001b[0m \u001b[48;2;141;160;203m and\u001b[0m \u001b[48;2;231;138;195m CAP\u001b[0m \u001b[48;2;166;216;84mITAL\u001b[0m \u001b[48;2;255;217;47mIZATION\u001b[0m \u001b[48;2;102;194;165m\n",
      "\u001b[0m \u001b[48;2;252;141;98m�\u001b[0m \u001b[48;2;141;160;203m�\u001b[0m \u001b[48;2;231;138;195m�\u001b[0m \u001b[48;2;166;216;84m�\u001b[0m \u001b[48;2;255;217;47m�\u001b[0m \u001b[48;2;102;194;165m�\u001b[0m \u001b[48;2;252;141;98m�\u001b[0m \u001b[48;2;141;160;203m�\u001b[0m \u001b[48;2;231;138;195m\n",
      "\u001b[0m \u001b[48;2;166;216;84mshow\u001b[0m \u001b[48;2;255;217;47m_\u001b[0m \u001b[48;2;102;194;165mtokens\u001b[0m \u001b[48;2;252;141;98m False\u001b[0m \u001b[48;2;141;160;203m None\u001b[0m \u001b[48;2;231;138;195m elif\u001b[0m \u001b[48;2;166;216;84m \u001b[0m \u001b[48;2;255;217;47m==\u001b[0m \u001b[48;2;102;194;165m \u001b[0m \u001b[48;2;252;141;98m>\u001b[0m \u001b[48;2;141;160;203m=\u001b[0m \u001b[48;2;231;138;195m else\u001b[0m \u001b[48;2;166;216;84m:\u001b[0m \u001b[48;2;255;217;47m two\u001b[0m \u001b[48;2;102;194;165m t\u001b[0m \u001b[48;2;252;141;98mabs\u001b[0m \u001b[48;2;141;160;203m:\u001b[0m \u001b[48;2;231;138;195m\"\u001b[0m \u001b[48;2;166;216;84m    \u001b[0m \u001b[48;2;255;217;47m\"\u001b[0m \u001b[48;2;102;194;165m Three\u001b[0m \u001b[48;2;252;141;98m t\u001b[0m \u001b[48;2;141;160;203mabs\u001b[0m \u001b[48;2;231;138;195m:\u001b[0m \u001b[48;2;166;216;84m \u001b[0m \u001b[48;2;255;217;47m\"\u001b[0m \u001b[48;2;102;194;165m       \u001b[0m \u001b[48;2;252;141;98m\"\u001b[0m \u001b[48;2;141;160;203m\n",
      "\u001b[0m \u001b[48;2;231;138;195m1\u001b[0m \u001b[48;2;166;216;84m2\u001b[0m \u001b[48;2;255;217;47m.\u001b[0m \u001b[48;2;102;194;165m0\u001b[0m \u001b[48;2;252;141;98m*\u001b[0m \u001b[48;2;141;160;203m5\u001b[0m \u001b[48;2;231;138;195m0\u001b[0m \u001b[48;2;166;216;84m=\u001b[0m \u001b[48;2;255;217;47m6\u001b[0m \u001b[48;2;102;194;165m0\u001b[0m \u001b[48;2;252;141;98m0\u001b[0m \u001b[48;2;141;160;203m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"facebook/galactica-1.3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;102;194;165m\u001b[0m \u001b[48;2;252;141;98m\n",
      "\u001b[0m \u001b[48;2;141;160;203mEnglish\u001b[0m \u001b[48;2;231;138;195mand\u001b[0m \u001b[48;2;166;216;84mC\u001b[0m \u001b[48;2;255;217;47mAP\u001b[0m \u001b[48;2;102;194;165mIT\u001b[0m \u001b[48;2;252;141;98mAL\u001b[0m \u001b[48;2;141;160;203mIZ\u001b[0m \u001b[48;2;231;138;195mATION\u001b[0m \u001b[48;2;166;216;84m\n",
      "\u001b[0m \u001b[48;2;255;217;47m�\u001b[0m \u001b[48;2;102;194;165m�\u001b[0m \u001b[48;2;252;141;98m�\u001b[0m \u001b[48;2;141;160;203m�\u001b[0m \u001b[48;2;231;138;195m�\u001b[0m \u001b[48;2;166;216;84m�\u001b[0m \u001b[48;2;255;217;47m�\u001b[0m \u001b[48;2;102;194;165m�\u001b[0m \u001b[48;2;252;141;98m\n",
      "\u001b[0m \u001b[48;2;141;160;203mshow\u001b[0m \u001b[48;2;231;138;195m_\u001b[0m \u001b[48;2;166;216;84mto\u001b[0m \u001b[48;2;255;217;47mkens\u001b[0m \u001b[48;2;102;194;165mFalse\u001b[0m \u001b[48;2;252;141;98mNone\u001b[0m \u001b[48;2;141;160;203melif\u001b[0m \u001b[48;2;231;138;195m==\u001b[0m \u001b[48;2;166;216;84m>=\u001b[0m \u001b[48;2;255;217;47melse\u001b[0m \u001b[48;2;102;194;165m:\u001b[0m \u001b[48;2;252;141;98mtwo\u001b[0m \u001b[48;2;141;160;203mtabs\u001b[0m \u001b[48;2;231;138;195m:\"\u001b[0m \u001b[48;2;166;216;84m  \u001b[0m \u001b[48;2;255;217;47m\"\u001b[0m \u001b[48;2;102;194;165mThree\u001b[0m \u001b[48;2;252;141;98mtabs\u001b[0m \u001b[48;2;141;160;203m:\u001b[0m \u001b[48;2;231;138;195m\"\u001b[0m \u001b[48;2;166;216;84m     \u001b[0m \u001b[48;2;255;217;47m\"\u001b[0m \u001b[48;2;102;194;165m\n",
      "\u001b[0m \u001b[48;2;252;141;98m1\u001b[0m \u001b[48;2;141;160;203m2\u001b[0m \u001b[48;2;231;138;195m.\u001b[0m \u001b[48;2;166;216;84m0\u001b[0m \u001b[48;2;255;217;47m*\u001b[0m \u001b[48;2;102;194;165m5\u001b[0m \u001b[48;2;252;141;98m0\u001b[0m \u001b[48;2;141;160;203m=\u001b[0m \u001b[48;2;231;138;195m6\u001b[0m \u001b[48;2;166;216;84m0\u001b[0m \u001b[48;2;255;217;47m0\u001b[0m \u001b[48;2;102;194;165m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating contextualized word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 384])\n"
     ]
    }
   ],
   "source": [
    "# Token level embeddings\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# load a tokenzier\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# load a model\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\", device_map=\"cuda\")\n",
    "\n",
    "# tokenize the sentence and generate output \n",
    "tokens = tokenizer(\"Hello world\", return_tensors=\"pt\").to(device)\n",
    "output = model(**tokens)[0]\n",
    "# tokens = tokenizer.encode(\"Hello world\", return_tensors=\"pt\").to(device)\n",
    "# output = model(tokens)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "Hello\n",
      " world\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for token in tokens[\"input_ids\"][0]:\n",
    "    print(tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "# Sentence level embeddings / whole docs\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Convert text to text embeddings\n",
    "vector = model.encode(\"Best movie ever!\")\n",
    "\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 1.0000001192092896),\n",
       " ('prince', 0.8236179351806641),\n",
       " ('queen', 0.7839043140411377),\n",
       " ('ii', 0.7746230363845825),\n",
       " ('emperor', 0.7736247777938843),\n",
       " ('son', 0.766719400882721),\n",
       " ('uncle', 0.7627150416374207),\n",
       " ('kingdom', 0.7542161345481873),\n",
       " ('throne', 0.7539914846420288),\n",
       " ('brother', 0.7492411136627197),\n",
       " ('ruler', 0.7434253692626953)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n",
    "# Other options include \"word2vec-google-news-300\"\n",
    "# More options at https://github.com/RaRe-Technologies/gensim-data\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "model.most_similar([model['king']], topn=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a song embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib import request\n",
    "\n",
    "# Get the playlist dataset file\n",
    "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the playlist dataset file. Skip the first two lines as\n",
    "# they only contain metadata\n",
    "\n",
    "lines = data.read().decode(\"utf-8\").split('\\n')[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove playlists with only one song\n",
    "playlists = [s.rstrip().split() for s in lines if len(s.split()) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load song metadata\n",
    "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
    "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
    "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
    "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
    "songs_df = songs_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gucci Time (w\\/ Swizz Beatz)</td>\n",
       "      <td>Gucci Mane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aston Martin Music (w\\/ Drake &amp; Chrisette Mich...</td>\n",
       "      <td>Rick Ross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Get Back Up (w\\/ Chris Brown)</td>\n",
       "      <td>T.I.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hot Toddy (w\\/ Jay-Z &amp; Ester Dean)</td>\n",
       "      <td>Usher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Whip My Hair</td>\n",
       "      <td>Willow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title      artist\n",
       "id                                                               \n",
       "0                        Gucci Time (w\\/ Swizz Beatz)  Gucci Mane\n",
       "1   Aston Martin Music (w\\/ Drake & Chrisette Mich...   Rick Ross\n",
       "2                       Get Back Up (w\\/ Chris Brown)        T.I.\n",
       "3                  Hot Toddy (w\\/ Jay-Z & Ester Dean)       Usher\n",
       "4                                        Whip My Hair      Willow"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist #0: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '2', '42', '43', '44', '45', '46', '47', '48', '20', '49', '8', '50', '51', '52', '53', '54', '55', '56', '57', '25', '58', '59', '60', '61', '62', '3', '63', '64', '65', '66', '46', '47', '67', '2', '48', '68', '69', '70', '57', '50', '71', '72', '53', '73', '25', '74', '59', '20', '46', '75', '76', '77', '59', '20', '43']\n",
      "Playlist #1: ['78', '79', '80', '3', '62', '81', '14', '82', '48', '83', '84', '17', '85', '86', '87', '88', '74', '89', '90', '91', '4', '73', '62', '92', '17', '53', '59', '93', '94', '51', '50', '27', '95', '48', '96', '97', '98', '99', '100', '57', '101', '102', '25', '103', '3', '104', '105', '106', '107', '47', '108', '109', '110', '111', '112', '113', '25', '63', '62', '114', '115', '84', '116', '117', '118', '119', '120', '121', '122', '123', '50', '70', '71', '124', '17', '85', '14', '82', '48', '125', '47', '46', '72', '53', '25', '73', '4', '126', '59', '74', '20', '43', '127', '128', '129', '13', '82', '48', '130', '131', '132', '133', '134', '135', '136', '137', '59', '46', '138', '43', '20', '139', '140', '73', '57', '70', '141', '3', '1', '74', '142', '143', '144', '145', '48', '13', '25', '146', '50', '147', '126', '59', '20', '148', '149', '150', '151', '152', '56', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '60', '176', '51', '177', '178', '179', '180', '181', '182', '183', '184', '185', '57', '186', '187', '188', '189', '190', '191', '46', '192', '193', '194', '195', '196', '197', '198', '25', '199', '200', '49', '201', '100', '202', '203', '204', '205', '206', '207', '32', '208', '209', '210']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Playlist #0: {playlists[0]}\")\n",
    "print(f\"Playlist #1: {playlists[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train our Word2Vec model\n",
    "model = Word2Vec(\n",
    "    playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title     Fade To Black\n",
      "artist        Metallica\n",
      "Name: 2172 , dtype: object\n"
     ]
    }
   ],
   "source": [
    "song_id = 2172\n",
    "print(songs_df.iloc[2172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations(song_id):\n",
    "    similar_songs = model.wv.most_similar(positive=str(song_id),topn=5)\n",
    "    similar_song_ids = [_id for _id, _ in similar_songs]\n",
    "    return songs_df.iloc[similar_song_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>Run To The Hills</td>\n",
       "      <td>Iron Maiden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586</th>\n",
       "      <td>The Last In Line</td>\n",
       "      <td>Dio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>November Rain</td>\n",
       "      <td>Guns N' Roses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>I Don't Know</td>\n",
       "      <td>Ozzy Osbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>Unchained</td>\n",
       "      <td>Van Halen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  title         artist\n",
       "id                                    \n",
       "2849   Run To The Hills    Iron Maiden\n",
       "5586   The Last In Line            Dio\n",
       "5549      November Rain  Guns N' Roses\n",
       "2976       I Don't Know  Ozzy Osbourne\n",
       "3167          Unchained      Van Halen"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_recommendations(song_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thellmbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
